{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание по теме \"Распознавание языка\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Соня Дурнева, БКЛ-151"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первый метод: частотные слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала при помощи пакета wikipedia скачаем по сто текстов для каждого из выбранных языков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=10):\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print('Skipping page {}'.format(page_name))\n",
    "            continue\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Бестөбе ауылдық округі\n",
      "Skipping page Тұщықұдық\n",
      "kk 98\n",
      "Skipping page Амальтея (значення)\n",
      "Skipping page Каупер\n",
      "Skipping page Алфей\n",
      "Skipping page 1384 (значення)\n",
      "Skipping page Семей (значення)\n",
      "Skipping page Базарська сільська рада\n",
      "uk 94\n",
      "Skipping page Ласінцы\n",
      "Skipping page Масюкоўшчына\n",
      "Skipping page Янавіцкі сельсавет\n",
      "be 97\n",
      "Skipping page Fernando Marías\n",
      "Skipping page Marie de Brabant\n",
      "Skipping page Lorier\n",
      "fr 97\n",
      "Skipping page Cunderdin, Western Australia\n",
      "Skipping page Hiroyuki Kobayashi (baseball)\n",
      "Skipping page Diecast\n",
      "Skipping page Dalzell\n",
      "Skipping page Center Street Historic District\n",
      "Skipping page Ionis\n",
      "en 94\n",
      "Skipping page Sven Pettersson (Ballsportler)\n",
      "Skipping page Tvrdy\n",
      "Skipping page SLE\n",
      "Skipping page RWK\n",
      "Skipping page Emerson\n",
      "Skipping page Leun (Begriffsklärung)\n",
      "Skipping page Schimmelpenninck\n",
      "Skipping page George Perkins\n",
      "Skipping page Newbigging\n",
      "Skipping page Kaula\n",
      "de 90\n"
     ]
    }
   ],
   "source": [
    "import wikipedia \n",
    "langs = ('kk', 'uk', 'be', 'fr', 'en', 'de')\n",
    "\n",
    "wiki_texts = {}\n",
    "for lang in langs:\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь на материале скачанных текстов создадим частотные списки для каждого из языков. Оставим топ-80 слов, не сопровождая их частотностью: при определении языка нам понадобятся только сами слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import collections\n",
    "import sys\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "def tokenize(text):\n",
    "    punct_extended = string.punctuation + '«»—…“”'\n",
    "    return [word.lower() for word in word_tokenize(text) if len(word) > 1 \n",
    "                                                    and not all([ch in punct_extended for ch in word])]\n",
    "\n",
    "word_freq_dicts = collections.defaultdict(lambda: 0)\n",
    "for lang in langs:\n",
    "    corpus = wiki_texts[lang]\n",
    "    freqs = collections.defaultdict(lambda: 0)\n",
    "    for article in corpus:\n",
    "        for word in tokenize(article.replace('\\n', '').lower()):\n",
    "            freqs[word] += 1\n",
    "    word_freq_dicts[lang] = sorted(freqs, key=lambda w: freqs[w], reverse=True)[:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Чтобы убедиться, что всё в порядке, выведем топ частотных слов французского языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'et', 'le', 'en', 'du', 'les', 'des', 'est', 'un', 'dans', 'par', 'au', 'une', 'sur', 'pour', 'il', 'qui', 'avec', 'sont', 'plus', 'que', 'son', 'elle', 'se', 'portail', 'sa', 'ou', 'paris', 'été', 'aux', 'ce', 'deux', 'ses', 'comme', 'ville', 'siècle', 'pays', 'nouvelle-zélande', 'cette', 'entre', 'mais', 'années', 'ont', 'france', 'pas', \"d'un\", \"d'une\", 'ne', 'barbara', 'juin', 'novembre', 'fait', 'timbre', 'premier', 'après', 'janvier', 'avril', 'mai', 'également', 'depuis', 'où', 'dont', 'mars', 'français', 'trois', 'février', 'septembre', 'sud', 'frf', 'décembre', 'on', 'très', 'première', 'était', 'the', 'même', 'argenteuil', 'aussi', 'juillet']\n"
     ]
    }
   ],
   "source": [
    "print(word_freq_dicts['fr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию *lang_detect_word( )*, которая будет принимать на вход текст и определять его язык. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def lang_detect_word(text):\n",
    "    text = tokenize(text)\n",
    "    options = {}\n",
    "    for lang in word_freq_dicts:\n",
    "        options[lang] = len(intersection(text, word_freq_dicts[lang]))\n",
    "    return(max(options.items(), key=operator.itemgetter(1))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, как работает функция на двух текстах – на белорусском языке и на английском."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "test_text1 = '- Добры дзень, сястра! - Добры дзень, Віця! - Я бачу, ты адна дома. А дзе бацькі? - Да бабулі паехалі абодва. Вернуцца толькі ўвечары. - А чаму яны мяне не папярэдзілі, што паедуць? Я, можа быць, і сам бы бабулю дачуўся. - А яны спачатку не плянавалі ехаць. Потым бабуля патэлефанавала, прасіла, каб яны прыехалі, дапамаглі ей. Ей трэба, каб тата старую дзіцячую ложачак сабраў і адвез, куды трэба. - Ложачак? Навошта? - А, Віця, ты ж яшчэ не ведаеш! У цеткі Іны і дзядзькі Вовы нарадзілася маленькая дзяўчынка, сення рана раніцай. - Ух, ты! - Грошай у іх цяпер мала, вось наша бабуля і вырашыла ім дапамагчы. Знайшла старую ложачак і дзіцячую ванначку, розныя рэчы для маляняці. Усе яны ў добрым стане. Цяпер хоча пераправіць іх цетцы Іне. - А як назавуць дзяўчынку? - Яшчэ не вырашылі, але хочуць, накшталт, Полинкой назваць. - Добра. Паліна - прыгожае імя. Цяпер у нас з\\'явілася траюрадная сястрычка! Подробнее - на Znanija.com - https://znanija.com/task/6101254#readmore'\n",
    "test_text2 = 'Apple trees are large if grown from seed. Generally apple cultivars are propagated by grafting onto rootstocks, which control the size of the resulting tree. There are more than 7,500 known cultivars of apples, resulting in a range of desired characteristics. Different cultivars are bred for various tastes and uses, including cooking, eating raw and cider production. Trees and fruit are prone to a number of fungal, bacterial and pest problems, which can be controlled by a number of organic and non-organic means. In 2010, the fruit\\'s genome was sequenced as part of research on disease control and selective breeding in apple production. Worldwide production of apples in 2014 was 84.6 million tonnes, with China accounting for 48% of the total.'\n",
    "print(lang_detect_word(test_text1))\n",
    "print(lang_detect_word(test_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Второй метод: частотные символьные n-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию, которая преобразовывает строку в массив n-грамм заданной длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice, tee\n",
    "\n",
    "def make_ngrams(text):\n",
    "    N = 3 # задаем длину n-граммы\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams = [''.join(x) for x in ngrams]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим частотные словари n-грамм аналогично первому методу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = collections.defaultdict(lambda: 0)\n",
    "\n",
    "\n",
    "ngram_freq_dicts = collections.defaultdict(lambda: 0)\n",
    "for lang in langs:\n",
    "    corpus = wiki_texts[lang]\n",
    "    freqs = collections.defaultdict(lambda: 0)\n",
    "    for article in corpus:\n",
    "        for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "            freqs[ngram] += 1\n",
    "    ngram_freq_dicts[lang] = sorted(freqs, key=lambda n: freqs[n], reverse=True)[:80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию *lang_detect_ngram( )*, аналогичную прошлой, но определяющей язык на основании пересечений с частотным словарём символьных n-грамм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "\n",
    "def lang_detect_ngram(text):\n",
    "    text = make_ngrams(text)\n",
    "    options = {}\n",
    "    for lang in ngram_freq_dicts:\n",
    "        options[lang] = len(intersection(text, ngram_freq_dicts[lang]))\n",
    "    return(max(options.items(), key=operator.itemgetter(1))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим работу функции на тех же текстах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "test_text1 = '- Добры дзень, сястра! - Добры дзень, Віця! - Я бачу, ты адна дома. А дзе бацькі? - Да бабулі паехалі абодва. Вернуцца толькі ўвечары. - А чаму яны мяне не папярэдзілі, што паедуць? Я, можа быць, і сам бы бабулю дачуўся. - А яны спачатку не плянавалі ехаць. Потым бабуля патэлефанавала, прасіла, каб яны прыехалі, дапамаглі ей. Ей трэба, каб тата старую дзіцячую ложачак сабраў і адвез, куды трэба. - Ложачак? Навошта? - А, Віця, ты ж яшчэ не ведаеш! У цеткі Іны і дзядзькі Вовы нарадзілася маленькая дзяўчынка, сення рана раніцай. - Ух, ты! - Грошай у іх цяпер мала, вось наша бабуля і вырашыла ім дапамагчы. Знайшла старую ложачак і дзіцячую ванначку, розныя рэчы для маляняці. Усе яны ў добрым стане. Цяпер хоча пераправіць іх цетцы Іне. - А як назавуць дзяўчынку? - Яшчэ не вырашылі, але хочуць, накшталт, Полинкой назваць. - Добра. Паліна - прыгожае імя. Цяпер у нас з\\'явілася траюрадная сястрычка! Подробнее - на Znanija.com - https://znanija.com/task/6101254#readmore'\n",
    "test_text2 = 'Apple trees are large if grown from seed. Generally apple cultivars are propagated by grafting onto rootstocks, which control the size of the resulting tree. There are more than 7,500 known cultivars of apples, resulting in a range of desired characteristics. Different cultivars are bred for various tastes and uses, including cooking, eating raw and cider production. Trees and fruit are prone to a number of fungal, bacterial and pest problems, which can be controlled by a number of organic and non-organic means. In 2010, the fruit\\'s genome was sequenced as part of research on disease control and selective breeding in apple production. Worldwide production of apples in 2014 was 84.6 million tonnes, with China accounting for 48% of the total.'\n",
    "print(lang_detect_ngram(test_text1))\n",
    "print(lang_detect_ngram(test_text2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь узнаем, с какой точностью определяется язык при использовании этих методов. Для этого загрузим новые тексты на тех же языках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kk 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page Кунжек\n",
      "Skipping page Гур'євський район\n",
      "Skipping page Кулаковка\n",
      "Skipping page Еберт\n",
      "Skipping page Діва (значення)\n",
      "uk 95\n",
      "Skipping page Іваноўшчына\n",
      "Skipping page Зарачаны\n",
      "Skipping page Духан\n",
      "Skipping page Ванкевіч\n",
      "Skipping page Гродзенскі павет (значэнні)\n",
      "Skipping page Ульянаўка\n",
      "Skipping page Мікалай Стэфанавіч Якушаў\n",
      "Skipping page Беседзь (значэнні)\n",
      "Skipping page Сконэ\n",
      "be 91\n",
      "Skipping page Uminski\n",
      "Skipping page Arcole (Italie)\n",
      "Skipping page West\n",
      "fr 97\n",
      "Skipping page Edward Edwards (actor)\n",
      "Skipping page Hugh McFadden (poet)\n",
      "Skipping page Surcharge\n",
      "Skipping page Sepon\n",
      "Skipping page Henryk Grabowski (athlete)\n",
      "Skipping page Robert Wilkin\n",
      "Skipping page Morozov\n",
      "en 93\n",
      "Skipping page Motorette\n",
      "Skipping page Nüssler\n",
      "Skipping page Vivat!\n",
      "Skipping page Kloster Fontaine\n",
      "Skipping page Procopé\n",
      "Skipping page John L. Mack\n",
      "Skipping page Fountain\n",
      "Skipping page Santa Brigida\n",
      "Skipping page Strength Through Joy\n",
      "Skipping page Fullen\n",
      "de 90\n"
     ]
    }
   ],
   "source": [
    "test_texts = {}\n",
    "for lang in langs:\n",
    "    test_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(test_texts[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(texts_dict):\n",
    "    errors_word_freq = 0\n",
    "    errors_ngram_freq = 0\n",
    "    texts = 0\n",
    "    for lang in texts_dict:\n",
    "        corpus = texts_dict[lang]\n",
    "        for article in corpus:\n",
    "            texts += 1\n",
    "            if lang != lang_detect_word(article):\n",
    "                errors_word_freq += 1\n",
    "            if lang != lang_detect_ngram(article):\n",
    "                errors_ngram_freq += 1\n",
    "    print('word frequency method error rate:', errors_word_freq/texts)\n",
    "    print('ngram frequency method error rate:', errors_ngram_freq/texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word frequency method error rate: 0.019434628975265017\n",
      "ngram frequency method error rate: 0.007067137809187279\n"
     ]
    }
   ],
   "source": [
    "test(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что точнее работает метод определения языка на основании частотного списка n-грамм – его error rate меньше, чем у метода определения языка по частотным словам."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
